{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utilities import *\n",
    "from sklearn.datasets import make_blobs, make_circles\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from scipy.signal import convolve2d\n",
    "from tqdm import tqdm\n",
    "# On specifie a matplotlib que l'on est en dark mode\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fonctions du réseau de neurones\n",
    "## Initialisation des paramètres"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a791285825af4dbe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fonction d'intialisation des parametres\n",
    "# entree : dimension (liste) : liste des dimensions des couches\n",
    "# sortie : parametres (dictionnaire) : dictionnaire des parametres initialises\n",
    "def initialisation(dimensions):\n",
    "    \n",
    "    parametres = {}\n",
    "    C = len(dimensions) # nombre de couches\n",
    "    \n",
    "    for c in range (1,C): # pour chaque couche (sauf la premiere)\n",
    "        parametres['W' + str(c)] = np.random.randn(dimensions[c], dimensions[c-1]) # initialisation aleatoire des poids\n",
    "        parametres['b' + str(c)] = np.random.randn(dimensions[c], 1) # initialisation des biais a zero\n",
    "\n",
    "    return parametres"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af143f40ef22c36b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test de la fonction d'initialisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d522e27950c19170"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test de la fonction d'initialisation\n",
    "parametres = initialisation([2, 32, 32, 1]) # Initialisation d'un RN avec :\n",
    "# 2 neurones en entree\n",
    "# 32 neurones sur chaque couche cachée\n",
    "# 1 neurone de sortie\n",
    "for k, v in parametres.items():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4e2636e4b31be89",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Propagation vers l'avant"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30c1f9123abbeac5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fonction de propagation avant\n",
    "# entree :  - X (np.array) : donnees d'entree\n",
    "#           - parametres (dictionnaire) : dictionnaire des parametres\n",
    "# sortie : activations (dictionnaire) : dictionnaire des activations\n",
    "\n",
    "def forward_propagation(X, parametres):\n",
    "    activations = {'A0': X} # initialisation : la premiere activation est l'entree\n",
    "    \n",
    "    C = len(parametres) // 2 # nombre de couches\n",
    "    \n",
    "    for c in range(1, C + 1): # pour chaque couche\n",
    "        Z = parametres['W' + str(c)].dot(activations['A' + str(c - 1)]) + parametres['b' + str(c)] # calcul de Z\n",
    "        activations['A' + str(c)] = 1 / (1 + np.exp(-Z)) # calcul de A\n",
    "    return activations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17aba8a2f45cad7a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test de la fonction de propagation avant"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b18b0ebacbcd6699"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test de la fonction de propagation avant\n",
    "X = np.array([[1, 2], [3, 4]]) # Donnees d'entree\n",
    "y = np.array([[0, 1]]) # Donnees de sortie\n",
    "activations = forward_propagation(X, parametres) # Propagation avant : avec en entree du premier neurone 1 et 3, et le deuxieme neurone 2 et 4.\n",
    "for k, v in activations.items():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15e6060da938bc74",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retropropagation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a15dc0dee01b89d0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def back_propagation(y, parametres, activations):\n",
    "\n",
    "  m = y.shape[1]\n",
    "  C = len(parametres) // 2\n",
    "\n",
    "  dZ = activations['A' + str(C)] - y\n",
    "  gradients = {}\n",
    "\n",
    "  for c in reversed(range(1, C + 1)):\n",
    "    gradients['dW' + str(c)] = 1/m * np.dot(dZ, activations['A' + str(c - 1)].T)\n",
    "    gradients['db' + str(c)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    if c > 1:\n",
    "      dZ = np.dot(parametres['W' + str(c)].T, dZ) * activations['A' + str(c - 1)] * (1 - activations['A' + str(c - 1)])\n",
    "\n",
    "  return gradients"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3752ce18545b7217",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test de la fonction de retropropagation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19e4cf32c1ba9002"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test de la fonction de retropropagation\n",
    "gradients = back_propagation(y, parametres, activations)\n",
    "for k, v in gradients.items():\n",
    "    print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d2fbe448730341",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mise à jour des paramètres (descente de gradient)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ce3083603fe99f5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def update(gradients, parametres, learning_rate):\n",
    "\n",
    "    C = len(parametres) // 2\n",
    "\n",
    "    for c in range(1, C + 1):\n",
    "        parametres['W' + str(c)] = parametres['W' + str(c)] - learning_rate * gradients['dW' + str(c)]\n",
    "        parametres['b' + str(c)] = parametres['b' + str(c)] - learning_rate * gradients['db' + str(c)]\n",
    "\n",
    "    return parametres"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34b9f02c627daf59",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prédiction de données"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e84605e4ec762425"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict(X, parametres):\n",
    "  activations = forward_propagation(X, parametres)\n",
    "  C = len(parametres) // 2\n",
    "  Af = activations['A' + str(C)]\n",
    "  return Af>= 0.5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c73e3aa9e406175",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Réseau de neurones profond (fonction globale)\n",
    "Dans cette fonction, on va entrainer un réseau de neurones profond sur un jeu de données donné.\n",
    "On commence par initialiser les paramètres du réseau, puis on effectue une descente de gradient pour minimiser la fonction de coût (log_loss).\n",
    "On affiche ensuite la courbe d'apprentissage (log_loss et accuracy) et on retourne les paramètres du réseau.\n",
    "\n",
    "Les paramètres de la fonction sont :\n",
    "- X : les données d'entrée\n",
    "- y : les données de sortie\n",
    "- hidden_layers : un tuple contenant le nombre de neurones par couche cachée\n",
    "- learning_rate : le taux d'apprentissage (Pour doser ce paramètre, il faut tester plusieurs valeurs, en général on commence par 0.1 puis on ajuste. Si le taux d'apprentissage est trop grand, l'algorithme peut diverger, si il est trop petit, l'algorithme peut mettre trop de temps à converger)\n",
    "- n_iter : le nombre d'itérations de la descente de gradient (plus il y a d'itérations, plus le modèle aura de chance de converger, mais plus le temps de calcul sera long)\n",
    "\n",
    "La fonction retourne un tuple contenant :\n",
    "- training_history : un tableau numpy contenant les valeurs de log_loss et d'accuracy à chaque itération\n",
    "- parametres : les paramètres du réseau de neurones entrainé"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6dc0af97c934550"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def deep_neural_network(X, y, hidden_layers = (16, 16, 16), learning_rate = 0.001, n_iter = 3000):\n",
    "    print(\"Learning rate : \", learning_rate, \"Number of iterations : \", n_iter)\n",
    "    \n",
    "    # initialisation parametres\n",
    "    dimensions = list(hidden_layers) # liste des dimensions des couches\n",
    "    dimensions.insert(0, X.shape[0]) # ajout de la dimension de l'entree\n",
    "    dimensions.append(y.shape[0]) # ajout de la dimension de la sortie\n",
    "    np.random.seed(1) # fixer la seed pour reproduire les resultats\n",
    "    parametres = initialisation(dimensions) # initialisation des parametres\n",
    "    \n",
    "    # affiche un shemas de la structure du reseau\n",
    "    print(\"Structure du reseau :\")\n",
    "    for i in range(0, len(dimensions)):\n",
    "        print(\"Couche \", i, \" : \", dimensions[i], \" neurones\")\n",
    "    print()\n",
    "    \n",
    "    # tableau numpy contenant les futures accuracy et log_loss\n",
    "    training_history = np.zeros((int(n_iter), 2))\n",
    "\n",
    "    C = len(parametres) // 2\n",
    "\n",
    "    # gradient descent\n",
    "    for i in tqdm(range(n_iter)):\n",
    "\n",
    "        activations = forward_propagation(X, parametres)\n",
    "        gradients = back_propagation(y, parametres, activations)\n",
    "        parametres = update(gradients, parametres, learning_rate)\n",
    "        Af = activations['A' + str(C)]\n",
    "\n",
    "        # calcul du log_loss et de l'accuracy\n",
    "        training_history[i, 0] = (log_loss(y.flatten(), Af.flatten()))\n",
    "        y_pred = predict(X, parametres)\n",
    "        training_history[i, 1] = (accuracy_score(y.flatten(), y_pred.flatten()))\n",
    "\n",
    "    # Plot courbe d'apprentissage\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_history[:, 0], label='train loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_history[:, 1], label='train acc')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return training_history, parametres"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72e474d81ea4ad81",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialisation d'un dataset 2D\n",
    "## On va commencer par initialiser un dataset 2D pour tester notre réseau de neurones.\n",
    "On va créer un dataset 2D avec 2 classes, puis on va entrainer notre réseau de neurones sur ce dataset. On va ensuite afficher la ligne de décision du réseau de neurones.\n",
    "\n",
    "Pour cela, on va :\n",
    " - Utiliser des fonctions de sklearn pour générer des datasets 2D.\n",
    "  - make_blobs : génère des points appartenant à 2 classes, les points de chaque classe sont regroupés.\n",
    "  - make_circles : génère des points appartenant à 2 classes, les points de chaque classe sont disposés en cercle.\n",
    "  - On va aussi créer un dataset 2D en générant des points aléatoires et en attribuant une classe en fonction de la distance de chaque point à l'origine.\n",
    "  - On va aussi créer un dataset 2D en générant des points aléatoires et en attribuant une classe en fonction de la position du point par rapport à une fonction cosinus.   \n",
    " - On va ensuite afficher les points du dataset avec la fonction scatter de matplotlib.pyplot, en coloriant les points en fonction de leur classe.\n",
    " - On va ensuite afficher la ligne de décision du réseau de neurones en utilisant la fonction contourf de matplotlib.pyplot, qui permet de tracer des lignes de niveau.\n",
    "\n",
    "### Note : \n",
    " - Les fonctions de génération de dataset génèrent aussi un dataset de test pour évaluer le réseau de neurones dans des conditions similaires à l'entrainement.\n",
    " - Les fonctions de génération de dataset retournent les données sous forme de matrices numpy, avec les points en colonnes et les classes en lignes.\n",
    "  - X est une matrice de dimension (2, n) où n est le nombre de points.\n",
    "  - y est une matrice de dimension (1, n) où n est le nombre de points."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff0c0db78fb9015f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dataset 2D de sklearn\n",
    "def dataset1_0():\n",
    "    X, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=0)\n",
    "    y = y.reshape((1, y.shape[0]))\n",
    "    X = X.T\n",
    "    print(X.shape, y.shape)\n",
    "    return X,y\n",
    "\n",
    "def dataset1_1():\n",
    "    X, y = make_circles(n_samples=100, noise=0.1, factor=0.3, random_state=0)\n",
    "    X = X.T\n",
    "    y = y.reshape((1, y.shape[0]))\n",
    "    print(X.shape, y.shape)\n",
    "    return X,y\n",
    "\n",
    "# Dataset 2D\n",
    "def dataset1_2():\n",
    "    X_train = np.random.randn(1000, 2)\n",
    "    X_test = np.random.randn(100, 2)\n",
    "    y_train = np.zeros((1000, 1))\n",
    "    y_test = np.zeros((100, 1))\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # si la distance de X[i] à l'origine est inférieure à 1 alors le point est de la classe 1 sinon de la classe 0\n",
    "        if np.linalg.norm(X_train[i]) < 1:\n",
    "            y_train[i] = 1\n",
    "        else:\n",
    "            y_train[i] = 0\n",
    "        if i < 100:\n",
    "            if np.linalg.norm(X_test[i]) < 1:\n",
    "                y_test[i] = 1\n",
    "            else:\n",
    "                y_test[i] = 0\n",
    "        \n",
    "\n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "    y_train = y_train.T\n",
    "    y_test = y_test.T\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "def dataset1_3(x1, x2, y1, y2):\n",
    "    X_train = np.zeros((1500, 2))\n",
    "    X_test = np.zeros((100, 2))\n",
    "    y_train = np.zeros((1500, 1))\n",
    "    y_test = np.zeros((100, 1))\n",
    "    \n",
    "    \n",
    "    for i in range(1500):\n",
    "        random_x = np.random.uniform(x1, x2)\n",
    "        random_y = np.random.uniform(y1, y2)\n",
    "        X_train[i] = np.array([random_x, random_y])\n",
    "        # si le point est au dessus de la fonction cosinus alors il est de la classe 1 sinon de la classe 0\n",
    "    \n",
    "        if random_y > np.cos(random_x):\n",
    "            y_train[i] = 1\n",
    "        else:\n",
    "            y_train[i] = 0\n",
    "        if i < 100:\n",
    "            random_x_test = np.random.uniform(x1, x2)\n",
    "            random_y_test = np.random.uniform(y1, y2)\n",
    "            X_test[i] = np.array([random_x_test, random_y_test])\n",
    "            if random_y_test > np.cos(random_x_test):\n",
    "                y_test[i] = 1\n",
    "            else:\n",
    "                y_test[i] = 0\n",
    "    \n",
    "    X_train = X_train.T\n",
    "    X_test = X_test.T\n",
    "    y_train = y_train.T\n",
    "    y_test = y_test.T\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "def affichage_train(X,y):\n",
    "    # affiche les points\n",
    "    plt.scatter(X[0, :], X[1, :], c=y.flatten(), cmap=plt.cm.coolwarm)\n",
    "    plt.show()\n",
    "    \n",
    "def affichage_data(X_train, y_train, X_test, y_test):\n",
    "    print(\"Train set :\")\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(np.unique(y_train, return_counts=True))\n",
    "    print()\n",
    "    print(\"Test set :\")\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    print(np.unique(y_test, return_counts=True))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2168d9645734f638",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialisation d'un dataset 2D\n",
    "## On va commencer par initialiser un dataset 2D pour tester notre réseau de neurones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fab93b6e976deb04"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# X_train,y_train = dataset1_0()\n",
    "# X_train,y_train = dataset1_1()\n",
    "# X_train, y_train, X_test, y_test = dataset1_2()\n",
    "X_train, y_train, X_test, y_test = dataset1_3(0, 10, -1.5, 1.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f11ec6fbebed332",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Affichage du dataset\n",
    "## On va afficher les points du dataset avec la fonction scatter de matplotlib.pyplot, en coloriant les points en fonction de leur classe."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5bf1a7cc5a8f7e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "affichage_train(X_train,y_train.T)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "554280d36893cea",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrainement du réseau de neurones\n",
    "\n",
    "## Entrainement sur le Dataset\n",
    "\n",
    "Nous allons maintenant entraîner notre réseau de neurones sur ce jeu de données.\n",
    "\n",
    "Pour cela, nous utiliserons la fonction `deep_neural_network` en fournissant les données d'entraînement. Nous spécifierons le nombre de neurones par couche cachée, le taux d'apprentissage et le nombre d'itérations de la descente de gradient.\n",
    "\n",
    "La fonction renverra les paramètres du réseau de neurones entraîné, ainsi qu'un tableau numpy contenant les valeurs de la perte logarithmique (log_loss) et de la précision (accuracy) à chaque itération.\n",
    "\n",
    "### Précision (Accuracy) :\n",
    "\n",
    "La précision est définie comme le nombre de prédictions correctes divisé par le nombre total de prédictions.\n",
    "\n",
    "### Perte Logarithmique (Log Loss) :\n",
    "\n",
    "La perte logarithmique (ou log_loss) est une mesure de l'erreur de prédiction. Elle est définie par la formule suivante :\n",
    "\n",
    "\\[ \\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\log(y_{\\text{pred}_i}) + (1 - y_i) \\log(1 - y_{\\text{pred}_i}) \\] \n",
    "\n",
    "Où :\n",
    "- \\( y \\) est le vecteur des vraies valeurs.\n",
    "- \\( y_{\\text{pred}} \\) est le vecteur des valeurs prédites par le réseau de neurones.\n",
    "- \\( n \\) est le nombre total d'exemples dans le jeu de données.\n",
    "\n",
    "Plus la perte logarithmique est proche de 0, meilleure est la performance du réseau de neurones dans la prédiction des données.\n",
    "### Si jamais la formule n'est pas affichée correctement : https://www.codecogs.com/latex/eqneditor.php"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf1e7abc8ed253c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Entrainement du reseau de neurones\n",
    "history, params = deep_neural_network(X_train, y_train, hidden_layers = (32, 32, 32, 32), learning_rate = 0.1, n_iter = 10000)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "490352db4f69c965",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prédiction des nouvelles données\n",
    "On va maintenant prédire les classes des points du dataset de test en utilisant la fonction `predict`.\n",
    "On va ensuite afficher la ligne de décision du réseau de neurones en utilisant la fonction `affichage_decision`.\n",
    "- Le premier plot affiche les points de train avec la ligne de decision\n",
    "- Le deuxieme plot affiche les points de test avec la ligne de decision (si la prédiction est juste le point est simbolisé en une croix sinon en un rond)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73c4b0a407a939df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Prédiction des nouvelles données\n",
    "predictions = predict(X_test, params)\n",
    "\n",
    "# Affiche la ligne de decision dans 2 plots :\n",
    "# - le premier plot affiche les points de train avec la ligne de decision\n",
    "# - le deuxieme plot affiche les points de test avec la ligne de decision (si la prédiction est juste le point est simbolisé en une croix sinon en un rond)\n",
    "def affichage_decision(X_train, y_train, X_test, predictions, y_test, params):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot pour les données d'entraînement\n",
    "    axes[0].scatter(X_train[0, :], X_train[1, :], c=y_train.flatten(), cmap=plt.cm.coolwarm)\n",
    "    axes[0].set_title('Entraînement')\n",
    "    plt.legend()\n",
    "\n",
    "    # Affichage de la ligne de décision pour les données d'entraînement\n",
    "    x = np.linspace(np.min(X_train[0, :]), np.max(X_train[0, :]), 100)\n",
    "    y = np.linspace(np.min(X_train[1, :]), np.max(X_train[1, :]), 100)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    Z = np.zeros(xx.shape)\n",
    "    for i in range(xx.shape[0]):\n",
    "        for j in range(xx.shape[1]):\n",
    "            Z[i, j] = predict(np.array([[xx[i, j]], [yy[i, j]]]), params)[0, 0]\n",
    "    axes[0].contourf(xx, yy, Z, alpha=0.5)\n",
    "\n",
    "    # Plot pour les données de test\n",
    "    axes[1].scatter(X_test[0, :], X_test[1, :], c=y_test.flatten(), cmap=plt.cm.coolwarm, label='Erreures de prédiction', edgecolors='k')\n",
    "    axes[1].set_title('Test')\n",
    "    plt.legend()\n",
    "\n",
    "    # Affichage de la ligne de décision pour les données de test\n",
    "    x = np.linspace(np.min(X_test[0, :]), np.max(X_test[0, :]), 100)\n",
    "    y = np.linspace(np.min(X_test[1, :]), np.max(X_test[1, :]), 100)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    Z = np.zeros(xx.shape)\n",
    "    for i in range(xx.shape[0]):\n",
    "        for j in range(xx.shape[1]):\n",
    "            Z[i, j] = predict(np.array([[xx[i, j]], [yy[i, j]]]), params)\n",
    "    axes[1].contourf(xx, yy, Z, alpha=0.5)\n",
    "\n",
    "    # Ajout des marqueurs pour les prédictions correctes et incorrectes\n",
    "    for i in range(X_test.shape[1]):\n",
    "        if predictions[0, i] == y_test[0, i]:\n",
    "            axes[1].scatter(X_test[0, i], X_test[1, i], marker='x', c='green')\n",
    "        else:\n",
    "            axes[1].scatter(X_test[0, i], X_test[1, i], marker='o', c='red')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "affichage_decision(X_train, y_train, X_test, predictions, y_test, params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a6e4f55abfd5389",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialisation d'un dataset (images)\n",
    "## On va maintenant initialiser un dataset d'images pour tester notre réseau de neurones.\n",
    "On utilise un dataset de 1200 images de 64x64 pixels en noir et blanc de chats et de chiens. Les images sont divisées en deux classes : les chats et les chiens. On prend 1000 images pour l'entrainement et 200 images pour le test."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc7442a530db4066"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def dataset3():\n",
    "    return load_data()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bde78c18bbe7d207",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = dataset3()\n",
    "def affiche_image (X,y): # affiche les 25 premières images avec leur label\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(X[i], cmap='gray')\n",
    "        plt.title(f\"Label: {y[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "def affiche_image_erreur (X,y, predictions):\n",
    "    plt.figure(figsize=(50, 25))\n",
    "    predictions_named = np.zeros(200, dtype=object)\n",
    "    for i in range(200):\n",
    "        if predictions[i] == 0:\n",
    "            predictions_named[i] = \"Chat\"\n",
    "        else:\n",
    "            predictions_named[i] = \"Chien\"\n",
    "        plt.subplot(10, 20, i + 1)\n",
    "        if y[i] == predictions[i]:\n",
    "            plt.imshow(X[i], cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(X[i], cmap='Reds')\n",
    "        plt.title(f\"Label: {predictions_named.T[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# on affiche les 200 premières images avec leur label, en rouge si la prédiction est fausse\n",
    "# on affiche en premier les images mal prédites (en parcoutant les 200 premières images, \n",
    "# on adapte l'affichage pour pas qu'il y ait trop de trous) puis les images bien prédites\n",
    "def affiche_image_erreur_sorted (X,y,predictions):\n",
    "    plt.figure(figsize=(50, 25))\n",
    "    predictions_named = np.zeros(X.shape[0], dtype=object)\n",
    "    nb_erreur = 0\n",
    "    for i in range(200):\n",
    "        if predictions[i] == 0: # 0 = chat\n",
    "            predictions_named[i] = \"Chat\"\n",
    "        else:\n",
    "            predictions_named[i] = \"Chien\"\n",
    "        if y[i] != predictions[i]: # si la prédiction est fausse\n",
    "            plt.subplot(10, 20, 200-nb_erreur)\n",
    "            nb_erreur += 1\n",
    "            plt.imshow(X[i], cmap='Reds')\n",
    "        else:\n",
    "            plt.subplot(10, 20, i+1-nb_erreur)\n",
    "            plt.imshow(X[i], cmap='gray')\n",
    "        plt.title(f\"Label: {predictions_named.T[i]}\")\n",
    "        plt.axis('off')\n",
    "    print(\"Nombre d'erreurs : \",nb_erreur,\"/\",200,\"\\nAccuracy de \",100 - nb_erreur/200*100,\"%\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe94ae910594241",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "affichage_data(X_train, y_train, X_test, y_test)\n",
    "affiche_image(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21990697c7ef7418",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modification des images\n",
    "## On va maintenant modifier les images pour les rendre plus facilement traitables par notre réseau de neurones.\n",
    " - On commence par normaliser les images, c'est-à-dire diviser les valeurs des pixels par 255 pour les ramener entre 0 et 1.\n",
    " - On aplatit les images, c'est-à-dire transformer les matrices 2D des images en vecteurs 1D.\n",
    " - On applique un filtre de Sobel sur les images pour mettre en évidence les contours des objets dans les images."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e10230befc094c5f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def filtreSobelProf(img):\n",
    "    # Ici le filtrage est 2D.\n",
    "    # Fitre de Sobel 2D en lignes\n",
    "    Gx = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
    "    # Filgre de Sobel 2D en colonnnes\n",
    "    Gy = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    " \n",
    "    # On crée une image 2D pour le filtre de Sobel en lignes\n",
    "    sobelX = convolve2d(img, Gx, 'same')\n",
    " \n",
    "    # On crée une image 2D pour le filtre de Sobel en colonnes\n",
    "    sobelY = convolve2d(img, Gy, 'same')\n",
    " \n",
    "    # On calcule la norme de ces 2 images\n",
    "    sobelN = np.zeros((img.shape[0], img.shape[1]), np.dtype(float))\n",
    "    for li in range(img.shape[0]):\n",
    "        for col in range(img.shape[1]):\n",
    "            sobelN[li, col] = np.sqrt(sobelX[li, col]*sobelX[li, col] + sobelY[li, col]*sobelY[li, col])\n",
    " \n",
    "    return sobelN\n",
    "\n",
    "\n",
    "# Normalize function\n",
    "def normalize_data(data):\n",
    "    return data / 255.0\n",
    "\n",
    "# Flatten function\n",
    "def flatten_data(data):\n",
    "    return data.reshape(data.shape[0], -1)\n",
    "\n",
    "# Application sur les images\n",
    "\n",
    "X_train_Sobel = np.zeros((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_test_Sobel = np.zeros((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "# Sobelize the images\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_train_Sobel[i] = filtreSobelProf(X_train[i])\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test_Sobel[i] = filtreSobelProf(X_test[i])\n",
    "\n",
    "# Normalize the train set and the test set\n",
    "X_train_normalized = normalize_data(X_train_Sobel)\n",
    "X_test_normalized = normalize_data(X_test_Sobel)\n",
    "\n",
    "# Flatten the images in the train set and the test set\n",
    "X_train_flattened = flatten_data(X_train_normalized)\n",
    "X_test_flattened = flatten_data(X_test_normalized)\n",
    "\n",
    "y_train_flattened = y_train.flatten()\n",
    "y_test_flattened = y_test.flatten()\n",
    "\n",
    "# reshape y_train and y_test\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "# Transpose the data\n",
    "X_train_flattened = X_train_flattened.T\n",
    "X_test_flattened = X_test_flattened.T\n",
    "\n",
    "# Print the shapes after normalization and flattening\n",
    "print(\"Train set after normalization and flattening:\")\n",
    "print(\"X_train shape:\", X_train_flattened.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print()\n",
    "print(\"Test set after normalization and flattening:\")\n",
    "print(\"X_test shape:\", X_test_flattened.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94b30d67b7c8f661",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrainement du réseau de neurones sur les images\n",
    "## On va maintenant entraîner notre réseau de neurones sur les images normalisées et aplaties."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5c215684a8729c2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train the model on the train_set\n",
    "training_history, trained_parameters = deep_neural_network(X_train_flattened, y_train, hidden_layers=(512,256,128,64,10), learning_rate=0.1, n_iter=500)\n",
    "\n",
    "# Evaluate the model on the test_set\n",
    "test_loss = training_history[-1, 0]  # Final training loss can be used as test loss\n",
    "print(\"Test set loss:\", test_loss)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4337ac4c1312f7b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prédiction des nouvelles données\n",
    "On va maintenant prédire les classes des images du dataset de test en utilisant la fonction `predict`.\n",
    "On va afficher l'accuracy du modèle et le nombre d'erreurs sur le dataset de test.\n",
    "On va ensuite afficher les images du dataset de test avec leur label, en rouge si la prédiction est fausse."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b4fe340a891b572"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def affiche_resultat(X, y, predictions):\n",
    "    # affiche l'accuarcy\n",
    "    print(\"Accuracy : \", accuracy_score(y, predictions))\n",
    "    affiche_image_erreur_sorted(X, y, predictions)\n",
    "\n",
    "# Predict the test set\n",
    "predictions = predict(X_test_flattened, trained_parameters)\n",
    "affiche_resultat(X_test, y_test_flattened, predictions.flatten())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa5ceab5be4019f9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test avec un réseau de neurones convolutif"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c91632a81b8cf893"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_normalized, y_train_flattened, test_size=0.2, random_state=42)\n",
    "\n",
    "# One hot encode the labels\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test_flattened)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train.reshape(-1, 64, 64, 1), y_train, validation_data=(X_val.reshape(-1, 64, 64, 1), y_val), epochs=100, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test_normalized.reshape(-1, 64, 64, 1), y_test)\n",
    "print(\"Test set loss:\", test_loss)\n",
    "print(\"Test set accuracy:\", test_accuracy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eed32f9d86ee46a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prédiction des nouvelles données\n",
    "On va maintenant prédire les classes des images du dataset de test en utilisant la fonction `predict`.\n",
    "On va afficher l'accuracy du modèle et le nombre d'erreurs sur le dataset de test.\n",
    "On va ensuite afficher les images du dataset de test avec leur label, en rouge si la prédiction est fausse."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c43e763779b4a726"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Predict the test set\n",
    "predictions = model.predict(X_test_normalized.reshape(-1, 64, 64, 1))\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "affiche_resultat(X_test, y_test_flattened, predictions)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a698ae45c951506",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "Dans cette étude, nous avons entraîné deux types de réseaux neuronaux : un réseau de neurones profond sur un jeu de données 2D et un réseau de neurones convolutif sur un jeu de données d'images de chats et de chiens. \n",
    "\n",
    "Nous avons réalisé plusieurs étapes :\n",
    "- Entraînement et affichage de la courbe d'apprentissage du réseau de neurones profond sur le jeu de données 2D, ainsi que l'affichage de la ligne de décision.\n",
    "- Affichage des images du jeu de données d'images de chats et de chiens avec leurs étiquettes, en colorant en rouge les prédictions incorrectes.\n",
    "- Entraînement d'un réseau de neurones convolutif sur le jeu de données d'images de chats et de chiens.\n",
    "- Affichage de l'accuracy du modèle ainsi que du nombre d'erreurs sur le jeu de données de test, et des images avec leurs étiquettes, en marquant en rouge les prédictions incorrectes.\n",
    "- Comparaison des performances des deux modèles.\n",
    "\n",
    "Il est clair que le réseau de neurones convolutif surpasse le réseau de neurones profond dans la classification d'images de chats et de chiens. Cette supériorité s'explique par la capacité des réseaux de neurones convolutifs à prendre en compte la structure spatiale des images. Ces réseaux utilisent des filtres pour extraire des caractéristiques des images, ce qui les rend plus performants dans ce type de tâches.\n",
    "\n",
    "En conclusion, les réseaux de neurones convolutifs sont plus adaptés pour le traitement des images que les réseaux de neurones profonds classiques."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7304c69239953adc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
